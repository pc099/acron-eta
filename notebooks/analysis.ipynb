{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asahi - Cost/Latency/Quality Analysis\n",
    "\n",
    "This notebook analyzes the results of the Asahi inference optimizer benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.10' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Load results\n",
    "data_dir = os.path.join('..', 'data')\n",
    "\n",
    "def load_json(filename):\n",
    "    path = os.path.join(data_dir, filename)\n",
    "    if os.path.exists(path):\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "baseline = load_json('baseline_results.json')\n",
    "optimized = load_json('optimized_results.json')\n",
    "\n",
    "if baseline and optimized:\n",
    "    print('Results loaded successfully')\n",
    "else:\n",
    "    print('Run: python main.py benchmark --mock  (from project root)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Comparison\n",
    "if baseline and optimized:\n",
    "    b_cost = baseline['total_cost']\n",
    "    o_cost = optimized['total_cost']\n",
    "    savings_pct = (b_cost - o_cost) / b_cost * 100 if b_cost > 0 else 0\n",
    "\n",
    "    print(f\"Baseline (All GPT-4): ${b_cost:.4f}\")\n",
    "    print(f\"Optimized (Smart):    ${o_cost:.4f}\")\n",
    "    print(f\"Savings:              {savings_pct:.1f}%\")\n",
    "    print(f\"Absolute Savings:     ${b_cost - o_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Usage Distribution\n",
    "if optimized:\n",
    "    print('\\nModel Usage (Optimized):')\n",
    "    for model, count in optimized.get('requests_by_model', {}).items():\n",
    "        cost = optimized.get('cost_by_model', {}).get(model, 0)\n",
    "        print(f'  {model}: {count} requests, ${cost:.4f} total cost')\n",
    "\n",
    "    print(f'\\nCache Hit Rate: {optimized.get(\"cache_hit_rate\", 0):.1%}')\n",
    "    print(f'Avg Latency:    {optimized.get(\"avg_latency_ms\", 0):.0f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Per Request Comparison\n",
    "if baseline and optimized:\n",
    "    b_per = b_cost / max(1, baseline['requests'])\n",
    "    o_per = o_cost / max(1, optimized['requests'])\n",
    "    print(f'Cost per request (Baseline): ${b_per:.6f}')\n",
    "    print(f'Cost per request (Optimized): ${o_per:.6f}')\n",
    "    print(f'Reduction: {((b_per - o_per) / b_per * 100):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency Comparison\n",
    "if baseline and optimized:\n",
    "    print(f'Avg Latency (Baseline):  {baseline.get(\"avg_latency_ms\", 0):.0f}ms')\n",
    "    print(f'Avg Latency (Optimized): {optimized.get(\"avg_latency_ms\", 0):.0f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Table\n",
    "if baseline and optimized:\n",
    "    print(f\"{'Metric':<30} {'Baseline':>12} {'Optimized':>12} {'Improvement':>12}\")\n",
    "    print('-' * 66)\n",
    "    print(f\"{'Total Cost':<30} ${b_cost:>11.4f} ${o_cost:>11.4f} {savings_pct:>11.1f}%\")\n",
    "    print(f\"{'Requests':<30} {baseline['requests']:>12} {optimized['requests']:>12} {'':>12}\")\n",
    "    print(f\"{'Avg Latency (ms)':<30} {baseline.get('avg_latency_ms',0):>12.0f} {optimized.get('avg_latency_ms',0):>12.0f} {'':>12}\")\n",
    "    print(f\"{'Cache Hit Rate':<30} {'N/A':>12} {optimized.get('cache_hit_rate',0):>11.1%} {'':>12}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
