"""
Contextual embedding engine for Asahi (Phase 2+).

Enhances embeddings by prepending an intelligent context summary
before embedding.  This differentiates semantically similar but
contextually different queries, improving cache hit accuracy.
"""

import logging
import os
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

from src.embeddings.engine import EmbeddingEngine

logger = logging.getLogger(__name__)


class ContextualEmbeddingEngine:
    """Generate context-enriched embeddings.

    Prepends a brief context summary (generated by an LLM or a
    template) to the text before embedding.  This produces embeddings
    that capture not just the text content but also its domain,
    intent, and constraints.

    Args:
        embedding_engine: Base embedding engine for vectorization.
        context_llm_model: LLM model to use for context generation.
            Defaults to ``"claude-3-5-haiku"``.
        use_mock: If ``True``, use template-based context instead of
            calling an LLM.
    """

    def __init__(
        self,
        embedding_engine: EmbeddingEngine,
        context_llm_model: str = "claude-3-5-haiku",
        use_mock: bool = True,
    ) -> None:
        self._embedder = embedding_engine
        self._context_model = context_llm_model
        self._use_mock = use_mock

    def generate_context(
        self,
        text: str,
        agent_id: Optional[str] = None,
        task_type: str = "general",
        quality_req: float = 3.5,
    ) -> str:
        """Generate a brief context summary for the given text.

        Args:
            text: The text to contextualize.
            agent_id: Optional agent identifier.
            task_type: Detected task type.
            quality_req: Minimum quality requirement.

        Returns:
            A 1-2 sentence context summary (20-50 tokens).
        """
        if self._use_mock:
            return self._mock_context(text, agent_id, task_type, quality_req)

        return self._llm_context(text, agent_id, task_type, quality_req)

    def embed_with_context(
        self,
        text: str,
        agent_id: Optional[str] = None,
        task_type: str = "general",
        quality_req: float = 3.5,
    ) -> Tuple[np.ndarray, str, str]:
        """Generate a context-enriched embedding.

        Prepends the context summary to the text, then embeds the
        combined string.

        Args:
            text: The text to embed.
            agent_id: Optional agent identifier.
            task_type: Detected task type.
            quality_req: Minimum quality requirement.

        Returns:
            Tuple of (embedding, context_summary, contextual_text).
        """
        context = self.generate_context(text, agent_id, task_type, quality_req)
        contextual_text = f"[Context: {context}] {text}"

        embedding = self._embedder.embed_text(contextual_text)

        logger.debug(
            "Contextual embedding generated",
            extra={
                "task_type": task_type,
                "context_length": len(context),
            },
        )

        return embedding, context, contextual_text

    def retrieve_with_context(
        self,
        query: str,
        vector_db: Any,
        agent_id: Optional[str] = None,
        task_type: str = "general",
        top_k: int = 5,
        threshold: float = 0.92,
    ) -> Optional[Dict[str, Any]]:
        """Search for similar entries using contextual embedding.

        Args:
            query: The query to search for.
            vector_db: Vector database to search.
            agent_id: Optional agent identifier.
            task_type: Detected task type.
            top_k: Number of results to return.
            threshold: Minimum similarity threshold.

        Returns:
            Best matching entry dict, or ``None`` if no match exceeds
            the threshold.
        """
        embedding, context, contextual_text = self.embed_with_context(
            query, agent_id, task_type
        )

        results = vector_db.query(
            embedding=embedding.tolist(), top_k=top_k
        )

        for result in results:
            if result.score >= threshold:
                return {
                    "vector_id": result.vector_id,
                    "score": result.score,
                    "metadata": result.metadata,
                    "context_used": context,
                }

        return None

    # ------------------------------------------------------------------
    # Internal
    # ------------------------------------------------------------------

    def _mock_context(
        self,
        text: str,
        agent_id: Optional[str],
        task_type: str,
        quality_req: float,
    ) -> str:
        """Generate a template-based context summary (no LLM call).

        Args:
            text: Input text.
            agent_id: Optional agent ID.
            task_type: Task type.
            quality_req: Quality requirement.

        Returns:
            Template-based context string.
        """
        # Extract key signals from the text
        text_preview = text[:100].strip()
        agent_part = f" from agent {agent_id}" if agent_id else ""

        return (
            f"This is a {task_type} query{agent_part} "
            f"requiring quality >= {quality_req}: {text_preview}"
        )

    def _llm_context(
        self,
        text: str,
        agent_id: Optional[str],
        task_type: str,
        quality_req: float,
    ) -> str:
        """Generate context using an LLM call.

        Args:
            text: Input text.
            agent_id: Optional agent ID.
            task_type: Task type.
            quality_req: Quality requirement.

        Returns:
            LLM-generated context string.
        """
        try:
            import anthropic

            client = anthropic.Anthropic(
                api_key=os.getenv("ANTHROPIC_API_KEY")
            )
            prompt = (
                f"Text to contextualize: {text[:500]}\n"
                f"Metadata: Agent: {agent_id}, Task: {task_type}, "
                f"Quality: {quality_req}\n"
                f"Provide 1-2 sentence summary explaining what this is "
                f"about, including the domain, intent, and any critical "
                f"constraints."
            )
            response = client.messages.create(
                model=self._context_model,
                max_tokens=100,
                messages=[{"role": "user", "content": prompt}],
            )
            return response.content[0].text if response.content else ""
        except Exception as exc:
            logger.warning(
                "LLM context generation failed; using template",
                extra={"error": str(exc)},
            )
            return self._mock_context(text, agent_id, task_type, quality_req)
